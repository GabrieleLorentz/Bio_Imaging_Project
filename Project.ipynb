{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SRrk7RFisDsi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717828559557,"user_tz":-120,"elapsed":1614,"user":{"displayName":"colab3","userId":"10141960595606914236"}},"outputId":"3f8e6479-b682-4bed-f7a4-f74c592458d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdzYplPrsFH0","executionInfo":{"status":"ok","timestamp":1717828573393,"user_tz":-120,"elapsed":13838,"user":{"displayName":"colab3","userId":"10141960595606914236"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec1bac5c-0e91-49bf-9db5-476c393f6b6f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pydicom in /usr/local/lib/python3.10/dist-packages (2.4.4)\n"]}],"source":["!pip install pydicom"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpBPVXgzsCOb"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pydicom\n","import os\n","import pydicom as dicom\n","from skimage import exposure\n","from skimage import transform\n","from skimage import io\n","from skimage import color\n","from scipy.ndimage import zoom\n","import cv2\n","from PIL import Image\n","\n","import csv\n","import copy\n","import glob\n","import torch\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, models\n","from torch import nn\n","from sklearn.metrics import f1_score"]},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{"id":"-poZjFLRIYVO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRqNthrVP-f5"},"outputs":[],"source":["path_train = '/content/drive/MyDrive/Progetto/CHAOS_Train_Sets/Train_Sets/MR'\n","path_test = '/content/drive/MyDrive/Progetto/CHAOS_Test_Sets/Test_Sets/MR'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSzQuNCTubb8"},"outputs":[],"source":["#loading dataset\n","dimensioni_vox = []\n","pazienti_img = []\n","pazienti_mask = []\n","\n","for path, subfolder, files in sorted(os.walk(path_train)):\n","  img_vol = []\n","  mask_vol = []\n","  if path.endswith('InPhase'):\n","    for filename in sorted(files):\n","      img_dcm_std = dicom.dcmread(os.path.join(path,filename))\n","      img = img_dcm_std.pixel_array\n","      img_vol.append (img)\n","\n","    z_space = img_dcm_std.SliceThickness\n","    x_space = img_dcm_std.PixelSpacing [0]\n","    y_space = img_dcm_std.PixelSpacing [1]\n","    vox_dim = (x_space, y_space, z_space)\n","    dimensioni_vox.append(vox_dim)\n","\n","    img_vol_raw = np.array(img_vol)\n","    pazienti_img.append(img_vol_raw)\n","\n","  if path.endswith('T1DUAL/Ground'):\n","    for filename in sorted(files):\n","      image = np.asarray(Image.open(os.path.join(path,filename)))\n","      mask_vol.append(image)\n","\n","    mask_vol_raw = np.array(mask_vol)\n","    pazienti_mask.append(mask_vol_raw)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alks-03YsUuy"},"outputs":[],"source":["#target parameters for resampling and resize\n","target_resolution = [1, 1, 1]\n","resolution = []\n","target_shape = [256,256,256]\n","\n","for vox in dimensioni_vox:\n","  scale_vector = (vox[0]/target_resolution[0], vox[1]/target_resolution[1], vox[2]/target_resolution[2])\n","  resolution.append(scale_vector)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yx9xO-j_scfw"},"outputs":[],"source":["#resampling and resizing of images\n","for i in range(0, 20):\n","  pazienti_img[i] = transform.rescale(pazienti_img[i], resolution[i], order=3, preserve_range=True, mode='constant')\n","\n","  scale_vector = (target_shape[0]/pazienti_img[i].shape[0], target_shape[1]/pazienti_img[i].shape[1], target_shape[2]/pazienti_img[i].shape[2])\n","  pazienti_img[i] = zoom (pazienti_img[i], scale_vector, order=3, mode='nearest')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTDEvbbBJPY-"},"outputs":[],"source":["#normalizing and equalizing images\n","norm_img = []\n","for x in pazienti_img:\n","  norm_image = cv2.normalize(x, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_8UC1)\n","  norm_img.append(norm_image)\n","\n","clahe = cv2.createCLAHE (clipLimit = 2.0, tileGridSize = (8, 8))\n","\n","for i in range(0, len(norm_img)):\n","  for j in range(0, len(norm_img[i])):\n","    norm_img[i][j] = clahe.apply (norm_img[i][j, :, :])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDjXMNezFkVz"},"outputs":[],"source":["#creation of a custom dataset\n","#splitting images for trading and validation\n","os.mkdir (\"/content/drive/MyDrive/Dataset4\")\n","os.mkdir (\"/content/drive/MyDrive/Dataset4/train\")\n","os.mkdir (\"/content/drive/MyDrive/Dataset4/train/image_train\")\n","os.mkdir (\"/content/drive/MyDrive/Dataset4/train/mask_train\")\n","os.mkdir (\"/content/drive/MyDrive/Dataset4/train/image_val\")\n","os.mkdir (\"/content/drive/MyDrive/Dataset4/train/mask_val\")\n","\n","z = 0\n","for i in range(1, len(norm_img)-3):\n","  for j in range(0, len(norm_img[i-1])):\n","    np.save(\"/content/drive/My Drive/Dataset4/train/image_train/\" + str(z) + \".npy\", norm_img[i-1][j]);\n","    z += 1\n","\n","z = 0\n","for i in range(16, len(norm_img)+1):\n","  for j in range(0, len(norm_img[i-1])):\n","    np.save(\"/content/drive/My Drive/Dataset4/train/image_val/\" + str(z) + \".npy\", norm_img[i-1][j]);\n","    z += 1\n","\n","del pazienti_img\n","del norm_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkqTOR9XEdFm"},"outputs":[],"source":["#resampling and resizing of masks\n","for i in range(0, 20):\n","  pazienti_mask[i] = transform.rescale(pazienti_mask[i], resolution[i], order=0, preserve_range=True, mode='constant')\n","\n","  scale_vector = (target_shape[0]/pazienti_mask[i].shape[0], target_shape[1]/pazienti_mask[i].shape[1], target_shape[2]/pazienti_mask[i].shape[2])\n","  pazienti_mask[i] = zoom (pazienti_mask[i], scale_vector, order=0, mode='nearest')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e25vWtV5Hw98"},"outputs":[],"source":["#splitting images for trading and validation\n","z = 0\n","for i in range(1, len(pazienti_mask)-3):\n","  for j in range(0, len(pazienti_mask[i-1])):\n","    np.save(\"/content/drive/My Drive/Dataset4/train/mask_train/\" + str(z) + \".npy\", pazienti_mask[i-1][j]);\n","    z += 1\n","\n","z = 0\n","for i in range(16, len(pazienti_mask)+1):\n","  for j in range(0, len(pazienti_mask[i-1])):\n","    np.save(\"/content/drive/My Drive/Dataset4/train/mask_val/\" + str(z) + \".npy\", pazienti_mask[i-1][j]);\n","    z += 1\n","\n","del pazienti_mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xpp71chdSIRS"},"outputs":[],"source":["os.mkdir (\"/content/drive/MyDrive/Dataset4/test\")\n","test = []\n","z = 0\n","for path, subfolder, files in sorted(os.walk(path_test)):\n","  if path.endswith('InPhase'):\n","    for filename in sorted(files):\n","      img_dcm_std = dicom.dcmread(os.path.join(path,filename))\n","      img = img_dcm_std.pixel_array\n","      np.save(\"/content/drive/My Drive/Dataset4/test/\" + str(z) + \".npy\", img);\n","      z += 1"]},{"cell_type":"markdown","source":["# Neural Network"],"metadata":{"id":"sPOroYzrIdv0"}},{"cell_type":"code","source":["# Helper function for visualizing some samples of the images/mask to controll overlap\n","def image_viewer(output_classes, image, mask):\n","\n","    x=mask[0].numpy()\n","    x*=output_classes # The function need the classes to be integer\n","    io.imshow(color.label2rgb(x, image[0].numpy(), bg_label=0)) # Set bkg transparent and shows only\n","                                                                #  other classes on top of the input png\n","plt.show()"],"metadata":{"id":"_VPzJiIrZcma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Helper function for model semantic segmentation output\n","def decode_segmap(image, nc=5):\n","    label_colors = np.array([(0, 0, 0),      # 0=sfondo\n","                             (63, 63, 63),    # 1=Liver\n","                             (126, 126, 126),    # 2=Right kidney\n","                             (189, 189, 189),    # 3=Left kidney\n","                             (252, 252, 252)   # 4=Spleen\n","                                        ])\n","\n","    r = np.zeros_like(image).astype(np.uint8)\n","    g = np.zeros_like(image).astype(np.uint8)\n","    b = np.zeros_like(image).astype(np.uint8)\n","\n","    for l in range(0, nc):\n","        idx = image == l\n","        r[idx] = label_colors[l, 0]\n","        g[idx] = label_colors[l, 1]\n","        b[idx] = label_colors[l, 2]\n","    rgb = np.stack([r, g, b], axis=2)\n","    return rgb"],"metadata":{"id":"QnNr483Xzhht"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset class loading input images and from two separate folders in numeric order\n","class SpineDataset(Dataset):\n","\n","    def __init__(self, image_dir, mask_dir, channels_img, channels_masks, transform = None):\n","        self.image_dir = image_dir #  Getting image folder\n","        self.mask_dir = mask_dir  # Getting mask forlder\n","        self.transform = transform # If there are transformtion to apply\n","        self.channels_img = channels_img\n","        self.channels_masks = channels_masks\n","    def __len__(self):\n","        return len(glob.glob(os.path.join(self.image_dir,'*.npy'))) # number of images found in the folder\n","\n","    def __getitem__(self,idx):\n","        img_name = os.path.join(self.image_dir,\"%d.npy\"%idx)\n","        img = np.load(img_name)\n","        tmp_img=np.ndarray((img.shape[0],img.shape[1], self.channels_img),dtype=np.uint8)\n","        for i in range(self.channels_img):\n","            tmp_img[:,:,i]=img\n","\n","        mask_name = os.path.join(self.mask_dir,\"%d.npy\"%idx)\n","        mask = np.load(mask_name)\n","        tmp_mask=np.ndarray((mask.shape[0],mask.shape[1], self.channels_masks),dtype=np.uint8)\n","        for i in range(self.channels_masks):\n","            tmp_mask[:,:,i]=mask\n","\n","        sample = {'image': tmp_img, 'mask': tmp_mask} # matched image and mask\n","\n","        if self.transform:\n","            sample=self.transform(sample) # Eventual transformation to be made on the input data\n","\n","        return sample"],"metadata":{"id":"8QvMkIBQDKUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom function to transform input data to tensor of shape Nc, H, W\n","class ToTensor(object):\n","\n","    def __call__(self, sample):\n","        img, mask = sample['image'], sample['mask']\n","        if img.ndim == 3:\n","            mask = mask.transpose((2, 0, 1))\n","        if img.ndim == 3:\n","            img = img.transpose((2, 0, 1))\n","        return {'image': torch.from_numpy(img).type(torch.FloatTensor),\n","                'mask': torch.from_numpy(mask).type(torch.FloatTensor)}"],"metadata":{"id":"VhoFcYanGIJm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input normalization to have image and mask in [0 1] based on data range and classes\n","class Normalize(object):\n","\n","    def __call__(self, sample):\n","        img, mask = sample['image'], sample['mask']\n","        return {'image': img/255,\n","                'mask': mask/4.0}"],"metadata":{"id":"de6K9ejzGMXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Freezing model to not retrain everything given the low dimensionality of the dataset\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"metadata":{"id":"Ef_VR8C9SEMp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load. of pretrained model and change of last layer to match the number of classes to be predicted\n","# Newly added layer will be trained\n","def createModel(output_classes):\n","    my_model = models.segmentation.fcn_resnet101(pretrained = True)\n","    set_parameter_requires_grad(my_model, feature_extracting = True)\n","    my_model.classifier[4] = nn.Conv2d(512, output_classes, kernel_size=(1, 1), stride=(1, 1))\n","    my_model.aux_classifier[4] = nn.Conv2d(256, output_classes, kernel_size=(1, 1), stride=(1, 1))\n","    my_model.train()\n","    return my_model"],"metadata":{"id":"qQvT4BmRnykg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function for model training\n","def train_model(model, criterion, dataloader, dataloader1, optimizer, metrics, bpath, num_epochs):\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = 1e10\n","    # Use gpu if available\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    # Logger\n","    fieldnames = ['epoch', 'Train_loss', 'Val_loss'] + \\\n","        [f'Train_{m}' for m in metrics.keys()] + \\\n","        [f'Val_{m}' for m in metrics.keys()]\n","\n","    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n","        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","        writer.writeheader()\n","\n","    for epoch in range(1, num_epochs+1):\n","        print('-' * 10)\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","\n","        batchsummary = {a: [0] for a in fieldnames}\n","\n","        for phase in ['Train', 'Val']:\n","            if phase == 'Train':\n","                model.train()  # Set model to training mode\n","                dataloaders=dataloader # Select dataset for training\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","                dataloaders=dataloader1 #Select dataste for validation\n","\n","            # Iterate over data.\n","            for sample in tqdm(iter(dataloaders)):\n","                inputs = sample['image'].to(device)\n","                masks = sample['mask'].to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'Train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs['out'], masks)\n","                    y_pred = outputs['out'].data.cpu().numpy().ravel()\n","                    y_true = masks.data.cpu().numpy().ravel()\n","                    for name, metric in metrics.items():\n","                        if name == 'f1_score':\n","                            batchsummary[f'{phase}_{name}'].append(\n","                                metric(y_true > 0, y_pred > 0, average='weighted'))\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'Train':\n","                        loss.backward()\n","                        optimizer.step()\n","            batchsummary['epoch'] = epoch\n","            epoch_loss = loss\n","            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n","            print('{} Loss: {:.4f}'.format(\n","                phase, loss))\n","        for field in fieldnames[3:]:\n","            batchsummary[field] = np.mean(batchsummary[field])\n","        print(batchsummary)\n","        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n","            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n","            writer.writerow(batchsummary)\n","            # deep copy the model\n","            if phase == 'Val' and loss < best_loss:\n","                best_loss = loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"metadata":{"id":"ySARUbMxNgx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","    path = '/content/drive/MyDrive/Dataset4/train/'\n","    channels_img = 3\n","    channels_masks = 5\n","    output_classes = 5\n","    batch_size = 32\n","    num_epochs = 5\n","    lr = 0.01\n","\n","    #dataset and loader\n","    transformed_dataset_train = SpineDataset(image_dir = path + 'image_train/', mask_dir = path + 'mask_train/', channels_img = channels_img, channels_masks = channels_masks,\n","                                            transform = transforms.Compose([Normalize(),ToTensor()]))\n","    transformed_dataset_val = SpineDataset(image_dir = path + 'image_val', mask_dir = path + 'mask_val/', channels_img = channels_img, channels_masks = channels_masks,\n","                                            transform = transforms.Compose([Normalize(),ToTensor()]))\n","\n","    # Visual evaluation of correct alignement between image and mask\n","    fig = plt.figure(figsize=(6,6))\n","\n","    sample = transformed_dataset_train[100]\n","    a = fig.add_subplot(1,3,1)\n","    a.axis('off')\n","    image_viewer(output_classes, **sample)\n","\n","    sample = transformed_dataset_train[170]\n","    a = fig.add_subplot(1,3,2)\n","    a.axis('off')\n","    image_viewer(output_classes, **sample)\n","\n","    sample = transformed_dataset_train[2000]\n","    a = fig.add_subplot(1,3,3)\n","    a.axis('off')\n","    image_viewer(output_classes, **sample)\n","\n","    plt.show()\n","\n","    dataloader = DataLoader(transformed_dataset_train, batch_size, shuffle = True)\n","    dataloader1 = DataLoader(transformed_dataset_val, batch_size, shuffle=False)\n","\n","    # Model creation and criterion, optimizer and metric\n","    my_model = createModel(output_classes)\n","    criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n","    optimizer = torch.optim.Adam(my_model.parameters(), lr)\n","    metrics = {'f1_score': f1_score}\n","    bpath = '/content/drive/MyDrive/Dataset4/train/'\n","    # Model training\n","    my_model_trained=train_model(my_model, criterion, dataloader, dataloader1, optimizer, metrics, bpath, num_epochs)\n","\n","    # Getting first batch of the training data to run the model and see its performance\n","    for i_batch, sample_batched in enumerate(dataloader):\n","        print(i_batch, sample_batched['image'].size(),\n","          sample_batched['mask'].size())\n","        break\n","\n","    # Visualization of the model output on one example image from training\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    sample_batched['image'] = sample_batched['image'].to(device)\n","    out = my_model_trained(sample_batched['image'])['out']\n","    out_c = out.cpu()\n","    for i in range(0,10):\n","      om = torch.argmax(out_c[i], dim=0).numpy()\n","      rgb = decode_segmap(om)\n","      plt.imshow(rgb)\n","      plt.show()"],"metadata":{"id":"LipIq2WvWP0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["main()"],"metadata":{"id":"kcu89aNsWQ7V","collapsed":true},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}